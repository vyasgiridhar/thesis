@INPROCEEDINGS{1,
  author={Yeh, Chin-Chia Michael and Zhu, Yan and Ulanova, Liudmila and Begum, Nurjahan and Ding, Yifei and Dau, Hoang Anh and Silva, Diego Furtado and Mueen, Abdullah and Keogh, Eamonn},
  booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)}, 
  title={Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and Shapelets}, 
  year={2016},
  volume={},
  number={},
  pages={1317-1322},
  keywords={Time series analysis;Approximation algorithms;Euclidean distance;Data mining;Indexes;Clustering algorithms;Text processing;Time Series;Similarity Joins;Motif Discovery},
  doi={10.1109/ICDM.2016.0179}
}

@INPROCEEDINGS{2,
  author={Zhu, Yan and Zimmerman, Zachary and Senobari, Nader Shakibay and Yeh, Chin-Chia Michael and Funning, Gareth and Mueen, Abdullah and Brisk, Philip and Keogh, Eamonn},
  booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)}, 
  title={Matrix Profile II: Exploiting a Novel Algorithm and GPUs to Break the One Hundred Million Barrier for Time Series Motifs and Joins}, 
  year={2016},
  volume={},
  number={},
  pages={739-748},
  keywords={Time series analysis;Earthquakes;Seismology;Scalability;Data mining;Graphics processing units;Indexes;Time series;joins;motifs;GPUs},
  doi={10.1109/ICDM.2016.0085}}

@misc{3,
      title={Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster}, 
      author={Nolan Dey and Gurpreet Gosal and Zhiming and Chen and Hemant Khachane and William Marshall and Ribhu Pathria and Marvin Tom and Joel Hestness},
      year={2023},
      eprint={2304.03208},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@INPROCEEDINGS{4,
author = {Zimmerman, Zachary and Kamgar, Kaveh and Senobari, Nader Shakibay and Crites, Brian and Funning, Gareth and Brisk, Philip and Keogh, Eamonn},
title = {Matrix Profile XIV: Scaling Time Series Motif Discovery with GPUs to Break a Quintillion Pairwise Comparisons a Day and Beyond},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3362721},
doi = {10.1145/3357223.3362721},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {74–86},
numpages = {13},
keywords = {AB-Join, Cloud Computing, Entomology, Fault-Tolerance, GPU, Matrix Profile, Numerical Optimization, SCAMP, Seismology, Self-Join, Spot Instance, Tiling, Time Series},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@inproceedings{5,
author = {Jacquelin, Mathias and Araya-Polo, Mauricio and Meng, Jie},
title = {Scalable distributed high-order stencil computations},
year = {2022},
isbn = {9784665454445},
publisher = {IEEE Press},
abstract = {Stencil computations lie at the heart of many scientific and industrial applications. Stencil algorithms pose several challenges on machines with cache based memory hierarchy, due to low re-use of memory accesses if special care is not taken to optimize them. This work shows that for stencil computation a novel algorithm that leverages a localized communication strategy effectively exploits the second generation Cerebras Wafer-Scale Engine (WSE-2), which has no cache hierarchy. This study focuses on a 25-point stencil finite-difference method for the 3D wave equation, a kernel frequently used in earth modeling as numerical simulation. In essence, the algorithm trades memory accesses for data communication and takes advantage of the fast communication fabric provided by the architecture. The algorithm ---historically memory-bound--- becomes compute-bound. This allows the implementation to achieve near perfect weak scaling, reaching up to 503 TFLOPs on WSE-2, a figure that only full clusters can eventually yield.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {30},
numpages = {13},
keywords = {wafer-scale, stencil computation, multi-processor architecture and micro-architecture, high performance computing, energy, distributed memory},
location = {Dallas, Texas},
series = {SC '22}
}

@inproceedings{6,
author = {Orenes-Vera, Marcelo and Sharapov, Ilya and Schreiber, Robert and Jacquelin, Mathias and Vandermersch, Philippe and Chetlur, Sharan},
title = {Wafer-Scale Fast Fourier Transforms},
year = {2023},
isbn = {9798400700569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577193.3593708},
doi = {10.1145/3577193.3593708},
abstract = {We have implemented fast Fourier transforms for one, two, and three-dimensional arrays on the Cerebras CS-2, a system whose memory and processing elements reside on a single silicon wafer. The wafer-scale engine (WSE) encompasses a two-dimensional mesh of roughly 850,000 processing elements (PEs) with fast local memory and equally fast nearest-neighbor interconnections.Our wafer-scale FFT (wsFFT) parallelizes a n3 problem with up to n2 PEs. At this point, a PE processes only a single vector of the 3D domain (known as a pencil) per superstep, where each of the three supersteps performs FFT along one of the three axes of the input array. Between supersteps, wsFFT redistributes (transposes) the data to bring all elements of each one-dimensional pencil being transformed into the memory of a single PE. Each redistribution causes an all-to-all communication along one of the mesh dimensions. Given the level of parallelism, the size of the messages transmitted between pairs of PEs can be as small as a single word. In theory, a mesh is not ideal for all-to-all communication due to its limited bisection bandwidth. However, the mesh interconnecting PEs on the WSE lies entirely on-wafer and achieves nearly peak bandwidth even with tiny messages.We analyze in detail computation and communication time, as well as the weak and strong scaling, using both FP16 and FP32 precision. With 32-bit arithmetic on the CS-2, we achieve 959 microseconds for 3D FFT of a 5123 complex input array using a 512 \texttimes{} 512 subgrid of the on-wafer PEs. This is the largest ever parallelization for this problem size and the first implementation that breaks the millisecond barrier.},
booktitle = {Proceedings of the 37th International Conference on Supercomputing},
pages = {180–191},
numpages = {12},
keywords = {FFT, wafer-scale, strong scaling, massive parallelization, explicit communication, mesh, network, bandwidth, programming models},
location = {Orlando, FL, USA},
series = {ICS '23}
}

@article{7,
title = {Efficient algorithms for Monte Carlo particle transport on AI accelerator hardware},
journal = {Computer Physics Communications},
volume = {298},
pages = {109072},
year = {2024},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2023.109072},
url = {https://www.sciencedirect.com/science/article/pii/S0010465523004174},
author = {John Tramm and Bryce Allen and Kazutomo Yoshii and Andrew Siegel and Leighton Wilson},
keywords = {High performance computing, Monte Carlo particle transport, Macroscopic cross section lookup, AI accelerators, Cerebras},
abstract = {The recent trend toward deep learning has led to the development of a variety of highly innovative AI accelerator architectures. One such architecture, the Cerebras Wafer-Scale Engine 2 (WSE-2), features 40 GB of on-chip SRAM, making it a potentially attractive platform for latency- or bandwidth-bound HPC simulation workloads. In this study, we examine the feasibility of performing continuous energy Monte Carlo (MC) particle transport on the WSE-2 by porting a key kernel from the MC transport algorithm to Cerebras's CSL programming model. New algorithms for minimizing communication costs and for handling load balancing are developed and tested. The WSE-2 is found to run 130 times faster than a highly optimized CUDA version of the kernel run on an NVIDIA A100 GPU—significantly outpacing the expected performance increase given the difference in transistor counts between the architectures.}
}

@misc{8,
    title = {The UCR Time Series Classification Archive},
    author = {Dau, Hoang Anh and Keogh, Eamonn and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan 
              and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Yanping and Hu, Bing 
              and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo, and Hexagon-ML},
    year = {2018},
    month = {8},
    note = {\url{https://www.cs.ucr.edu/~eamonn/time_series_data_2018/}}
}

@misc{9,
      title={Massively scalable stencil algorithm}, 
      author={Mathias Jacquelin and Mauricio Araya-Polo and Jie Meng},
      year={2022},
      eprint={2204.03775},
      archivePrefix={arXiv},
      primaryClass={cs.MS}
}

