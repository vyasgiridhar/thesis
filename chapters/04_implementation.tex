% !TEX root = ../main.tex

\chapter{Implementation}\label{chapter:implementation}

The following chapter describes how the design described in Section 3 is realized as an Cerebras accelerated application. This chapter uses the Cerebras SDK conventions and/or syntax as the underlying work targets the Cerebras WSE-2 hardware.
Before potraying concrete implementation choices, we briefly introduce the reader to the Cerebras SDK and the WSE programming paradigm.
\clearpage
\section{WSE Programming Environment}

To develop code for the WSE, we write device code in CSL, and host code in Python. we then compile the device code, and run the program on either the Cerebras fabric simulator, or the actual network-attached device. The host code is responsible for copying data to and from the device, and launching discrete programs referred to as kernels.

\subsection{Execution Model}

As described in Section 3.1, the design is split between a \texttt{Host Code} and a \texttt{CSL Kernel}. The \texttt{Host Code} is written in Python and uses the Cerebras SDK. It provides a host runtime known as \textit{SdkRuntime}, and associated functionality in the CSL memcpy library, to load programs, launch functions, and transfer data on and off the Wafer-Scale Engine.

The interaction between Host Code and kernel consists of four distinct stages:

\begin{enumerate}
    \item The \texttt{Host Code}  prepares the data for the given timeseries, sets up the kernel, connects to the Device/Simulator and setups up the required amount of Computing resource (\texttt{Resource Rectangle}) and transfers it to the device.
    \item The execution of the kernel function on the PEs is triggered by the Host code.
    \item Once all the PEs have performed the required computation, It triggers the host that it's ready to accept commands from the Host Code.
    \item Finally, the Host code copies back the result into host memory.  
\end{enumerate}

\subsection{Build Process}

The build process is resembles a standard compilation for the device code which is compiled using the \texttt{cslc} compiler toolchain. It outputs binaries which are then picked up by the Python Host Code to be transfered to the device to all the allocated PEs.\\
The build process is quite straightforward but is accompanied with a build script written with GNU make-files. The underlying work provides a more convenient Python wrapper that builds and runs the code on the Simulator/Device and verify the results.

\clearpage
\section{Host Code}

In this section, we discuss the implementation specifics of the \texttt{Host Code} outlined in Section 3.1. As previously mentioned, the Host code starts by loading the input time series from a file specified via a command line argument $(--file_a)$. The Host code also allows us to process only specific tiles for computation or the entire time series. It then performs the required pre-computation for the two time series. The Code then initiates a \texttt{SdkRuntime} from the cerebras library which connects to and initializes the device or starts the Cerebras fabric simulator. The \texttt{SdkRuntime} populates the allocated PEs with the binaries and provides access to symbols exposed by the kernel program.

The Host Code then prepares the data required for computation \[T_a, T_b, dg_a, dg_b, df_a, df_b, inv_a, inv_b, args\] and pushes them to the device. One caveat here is the fact that the arrays declared in the kernel are sized statically and therefore require padding from the host with precise sizes.

\begin{figure}[!ht]
    \centering
    \begin{minted}[mathescape, breaklines,frame=single, fontsize=\footnotesize]{python}
    runner.memcpy_h2d(T_A_symbol, np.array(T_a, dtype=np.float32),
                      0, 0, width, height,
                      MAX_TILE_SIZE + WINDOW, streaming=False,
                      order=MemcpyOrder.ROW_MAJOR,
                      data_type=MemcpyDataType.MEMCPY_32BIT,
                      nonblock=False)
    runner.memcpy_h2d(T_B_symbol, np.array(T_b, dtype=np.float32),
                      0, 0, width, height,
                      MAX_TILE_SIZE + WINDOW, streaming=False,
                      order=MemcpyOrder.ROW_MAJOR,
                      data_type=MemcpyDataType.MEMCPY_32BIT,
                      nonblock=False)
\end{minted}
\caption{Memory Copy to the device, The SDK allows us to define the order of exporting data along the PEs and the symbol to which the data needs to be transferred}\label{code:memcpy_to}
\end{figure}

\begin{figure}[!ht]
    \centering
    \begin{minted}[mathescape, breaklines,frame=single, fontsize=\footnotesize]{python}
    runner.launch('compute', nonblock=False)
\end{minted}
\end{figure}

Once the data is transfered, The kernel function $compute()$ is invoked. Once the kernel function is complete and the device to recieve more commands, The tile aggregates are then extracted from the device and merged based on the tile positions.

\begin{figure}[!ht]
    \centering
    \begin{minted}[mathescape, breaklines,frame=single, fontsize=\footnotesize]{python}
    runner.memcpy_d2h(MP_A_result, MP_A_symbol,
                      0, 0, width, height,
                      MAX_TILE_SIZE - WINDOW + 1, streaming=False, 
                      order=MemcpyOrder.ROW_MAJOR,
                      data_type=MemcpyDataType.MEMCPY_32BIT,
                      nonblock=False)
    runner.memcpy_d2h(MP_B_result, MP_B_symbol,
                      0, 0, width, height,
                      MAX_TILE_SIZE - WINDOW + 1, streaming=False,
                      order=MemcpyOrder.ROW_MAJOR,
                      data_type=MemcpyDataType.MEMCPY_32BIT,
                      nonblock=False)
\end{minted}
\caption{Memory Copy from the device, Note the explicit PEs width and height declaration}\label{code:memcpy_from}
\end{figure}

\section{Kernel Implementation}

As elaborated in Section 3.4, The Kernel implements a tiled version of Matrix Profiling algorithm and each PE is capable of computing the row- and column-aggregates of a single tile with a upper bound on \texttt{Tile Size} of 100. Each kernel in addition gets the following args for computation.\\
\(n_x, n_y\): They represent the actual boundaries of the tile and impact the runtime of the kernel. 

\(exclusion\_lower_u, exclusion\_upper_u, exclusion\_lower_b, exclusion\_upper_b\): The exclusion boundaries provide a exclusion boundaries for the two triangles that are part of the tile and are taken into account when setting the kernel 

\(full\_tile\): The specifies the kernel if it has to compute just the upper triangle or both upper and the bottom triangle of the square tile.

The Kernel then computes the current rows $\overline{QT}$ for the two time series at that particular point using the following function.

\begin{verbatim}
    fn compute_cov(T_a: [*]f32, T_b: [*]f32) [*]f32
\end{verbatim}

Once $\overline{QT}$ is calculated then the kernel function is launched which calculates the row- and column-aggregates with the appropriate precomputed factors $dg, df$ and $inv$ to the following kernel function.

\begin{verbatim}
    fn kernel(norm_a: [*]f32, norm_b: [*]f32,
              dg_a: [*]f32, dg_b: [*]f32,
              df_a: [*]f32, df_b: [*]f32,
              P_a: *[N]f32, P_i_a: *[N]i32,
              P_b: *[N]f32, P_i_b: *[N]i32,
              cov: [*]f32, n_x: u8, n_y: u8,
              exclusion_lower: u8, exclusion_upper: u8) void
\end{verbatim}



The precomputed factors are flipped based on weather it is computing the upper tile or the bottom tile.

Once the entire tile is completed, the \texttt{rowAggregates} and \texttt{columnAggregates} are then transfered to the host from all the PEs. The Host then reorders the aggregates based on tile information into a single Matrix Profile.